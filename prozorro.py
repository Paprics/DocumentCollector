import random
import time
from functools import wraps
from playwright.sync_api import sync_playwright, Page
from playwright.sync_api import TimeoutError as PWTimeout
from utils.funcs import save_files_as_html


def retry(attempts=100, delay_range=(2, 3)):
    def decorator(func):
        @wraps(func)
        def wrapper(page, *args, **kwargs):
            for attempt in range(1, attempts + 1):
                result = func(page, *args, **kwargs)

                if result is not None:
                    print(f"[INFO] {func.__name__} ‚Äî —É—Å–ø–µ—Ö —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt}")
                    return result

                print(f"[WARN] –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{attempts} ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç None")

                if attempt < attempts:
                    sleep_time = random.uniform(*delay_range)
                    time.sleep(sleep_time)
                    try:
                        page.reload()
                    except Exception as e:
                        print(f"[WARN] reload failed: {e}")

            print(f"[ERROR] {func.__name__} ‚Äî –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
            try:
                page.close()
            except Exception:
                pass
            return None

        return wrapper

    return decorator


def fetch_documents(documents):
    result = []
    for i in range(documents.count()):
        document_item = documents.nth(i)
        links = document_item.locator('a').all()
        for link in links:
            spans = link.locator('span').all_text_contents()
            title = spans[0] if spans else link.text_content() or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            href = link.get_attribute('href') or "–ë–µ–∑ —Å—Å—ã–ª–∫–∏"
            result.append((title, href))
    return result


def process_participant(page: Page, participant_block):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ —Ç–µ–Ω–¥–µ—Ä–∞.
    """
    try:
        participant_block.scroll_into_view_if_needed()
        page.wait_for_timeout(300)

        # OPEN ACCORDION
        accordion_triger = participant_block.locator('xpath=.//button[contains(@class, "accordion__trigger")]').first
        accordion_triger.click()
        page.wait_for_timeout(200)

        # –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–∫—Ä—ã—Ç—å –≤—Å–µ –¥–æ–∫–µ–º–µ–Ω—Ç—ã
        try:
            participant_block.locator('xpath=.//span[@class="select__text"]').click()
            page.wait_for_timeout(200)
            participant_block.locator('xpath=.//div[@class="select__element"][last()]').click()
            page.wait_for_timeout(200)

        except Exception as e:
            print(f"[ERROR] –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫ –∏–ª–∏ < 5")

        try:
            document_block = participant_block.locator('xpath=.//div[@class="documents"]/div/ul')
            print(f'[INFO] –Ω–∞ —Ç–µ–Ω–¥–µ–¥–µ –Ω–∞–π–¥–µ–Ω–æ {document_block.count()} –¥–æ–∫–µ–º–µ–Ω—Ç–æ–≤')

            # print(f'[DEBUG TIMEE 5S]')
            # page.wait_for_timeout(5000)

            if document_block.count() > 0:
                result = fetch_documents(document_block)
                save_files_as_html(url=page.url, files=result)
            else:
                print(f'[WARN] –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ == 0', '|', document_block.count())

        except Exception as e:
            print(f'[ERROR üî¥üî¥üî¥üî¥] (–ù–ï –ü–†–ï–î–í–ò–î–ï–ù–ê–Ø –û–®–ò–ë–ö–ê –ü–û–°–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í) {e}')

        # CLOSE ACCORDION
        accordion_triger = participant_block.locator('xpath=.//button[contains(@class, "accordion__trigger")]').first
        accordion_triger.scroll_into_view_if_needed()
        page.wait_for_timeout(200)
        accordion_triger.click()
        page.wait_for_timeout(200)

    except Exception as e:
        print(f'[ERROR üî¥üî¥üî¥üî¥] (–ù–ï –ü–†–ï–î–í–ò–î–ï–ù–ê–Ø –û–®–ò–ë–ö–ê) {e}')


@retry(attempts=100)
def process_tender_page(page, tender_url: str):
    url = f'https://prozorro.gov.ua/uk{tender_url}'
    page.goto(url)

    # –ñ–¥—ë–º title
    try:
        title_locator = page.locator('//h2[contains(@class, "title--large")]')
        title_locator.first.wait_for(timeout=5000)
        title_text = title_locator.first.text_content()
        if not title_text:
            return None
    except PWTimeout:
        return None

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
    try:
        participants_locator = page.locator(
            '//section[contains(@class, "register")]//div[contains(@class, "accordion")]'
        )

        participants_locator.first.wait_for(timeout=2500)
        participants_count = participants_locator.count()

        if participants_count == 0:
            print(f'[INFO] —Ç–µ–Ω–¥–µ—Ä {url} ‚Äî –Ω–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
            return []
        else:
            print(f'[INFO] —Ç–µ–Ω–¥–µ—Ä {url} ‚Äî {participants_count} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
            # participants_locator.first.scroll_into_view_if_needed()
            return participants_locator
    except PWTimeout:
        print(f'[DEB] —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ üòå{page.url}')
        page.close()
        return []


@retry(attempts=100)
def fetch_tender_links(page, page_index: int):
    url = (
        f"https://prozorro.gov.ua/uk/search/tender?"
        f"cpv=34110000-1&page={page_index}&status=complete&sort=publication_date,asc"
    )

    page.goto(url)

    try:
        links_locator = page.locator(
            '//ul[@class="search-result__list"]//a[contains(@class,"item-title__title")]'
        )
        links_locator.first.wait_for(timeout=10000)

        links = links_locator.evaluate_all(
            "els => els.map(e => e.getAttribute('href'))"
        )

        links = [l for l in links if l]
        return links or None

    except PWTimeout:
        return None


def run_scraper(start_page: int, end_page: int, headless: bool):
    """
    –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª: –ø–µ—Ä–µ–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–Ω–¥–µ—Ä–æ–≤.
    """
    with sync_playwright() as pw:
        browser = pw.chromium.launch(
            headless=headless,
            args=["--no-sandbox", "--disable-setuid-sandbox"]
        )
        context = browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/117.0.0.0 Safari/537.36"
            ),
            locale="uk-UA"
        )

        page = context.new_page()

        for page_index in range(start_page, end_page + 1):
            print(f"[INFO] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_index}")

            try:
                # –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤
                tender_links = fetch_tender_links(page, page_index)
                print(f'[DED] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index} —Å–æ–±—Ä–∞–Ω—ã —Ç–µ–Ω–¥–µ—Ä—ã - {len(tender_links)} |)')
            except Exception as e:
                print(f'[ERROR] {e}')

            # –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–Ω–¥–µ—Ä–∞–º
            for tender_url in tender_links:

                page = context.new_page()

                # –ü–æ–ª—É—á–∏—Ç—å –Ω–∞ —Å—Ç—Ä. —Ç–µ–Ω–¥–µ—Ä–∞ —Å–ø–∏—Å–æ–∫ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
                try:
                    participants = process_tender_page(page, tender_url)

                    # –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —É—á–∞—Å–∏–Ω–∏–∫–∞–º
                    if not participants:  # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –ø—É—Å—Ç–æ–π(–Ω–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)
                        continue
                    for i in range(participants.count()):
                        print(f'[INFO] –û–±—Ä–∞–±–æ—Ç–∫–∞ {i + 1} —É—á–∞—Å—Ç–Ω–∏–∫–∞ —Ç–µ–Ω–¥–µ—Ä–∞ - {page.url}')
                        participant_block = participants.nth(i)
                        process_participant(page, participant_block)



                    page.close()
                    print('----' * 100)

                # #         save_results(documents, tender_url)
                except Exception as e:
                    print(f'[ERROR] {e}')


if __name__ == "__main__":
    HEADLESS = False
    START_PAGE = 293
    END_PAGE = 300
    run_scraper(START_PAGE, END_PAGE, HEADLESS)
    # MAX_CONCURRENT_TENDERS = 1   # –Ω–µ —É–¥–∞–ª—è—Ç—å
