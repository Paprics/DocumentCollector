import asyncio
import random
import time
from functools import wraps
from typing import Optional, List, Tuple

from playwright.async_api import async_playwright, Page, TimeoutError as PWTimeout, BrowserContext

from utils.funcs import save_files_as_html   # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–∂–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–ª–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è ‚Äî –∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏


def retry(attempts: int = 100, delay_range: tuple = (2, 3)):
    """
    –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(page: Page, *args, **kwargs):
            for attempt in range(1, attempts + 1):
                result = await func(page, *args, **kwargs)
                if result is not None:
                    print(f"[INFO] {func.__name__} ‚Äî —É—Å–ø–µ—Ö —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt}")
                    return result
                print(f"[WARN] –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{attempts} ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç None")
                if attempt < attempts:
                    sleep_time = random.uniform(*delay_range)
                    await asyncio.sleep(sleep_time)
                    try:
                        await page.reload()
                    except Exception as e:
                        print(f"[WARN] reload failed: {e}")
            print(f"[ERROR] {func.__name__} ‚Äî –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
            try:
                await page.close()
            except Exception:
                pass
            return None
        return wrapper
    return decorator


async def fetch_documents(documents) -> List[Tuple[str, str]]:
    result = []
    count = await documents.count()
    for i in range(count):
        document_item = documents.nth(i)
        links = await document_item.locator('a').all()
        for link in links:
            spans = await link.locator('span').all_text_contents()
            title = spans[0] if spans else (await link.text_content() or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            href = await link.get_attribute('href') or "–ë–µ–∑ —Å—Å—ã–ª–∫–∏"
            result.append((title, href))
    return result


async def process_participant(page: Page, participant_block):
    try:
        await participant_block.scroll_into_view_if_needed()
        await page.wait_for_timeout(300)

        # OPEN ACCORDION
        accordion_trigger = participant_block.locator('xpath=.//button[contains(@class, "accordion__trigger")]').first
        await accordion_trigger.click()
        await page.wait_for_timeout(200)

        # –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–∫—Ä—ã—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        try:
            await participant_block.locator('xpath=.//span[@class="select__text"]').click(timeout=4000)
            await page.wait_for_timeout(200)
            await participant_block.locator('xpath=.//div[@class="select__element"][last()]').click()
            await page.wait_for_timeout(200)
        except Exception as e:
            print(f"[ERROR] –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫ –∏–ª–∏ < 5 ‚Üí {e}")

        try:
            document_block = participant_block.locator('xpath=.//div[@class="documents"]/div/ul')
            doc_count = await document_block.count()
            print(f'[INFO] –Ω–∞ —Ç–µ–Ω–¥–µ—Ä–µ –Ω–∞–π–¥–µ–Ω–æ {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')

            if doc_count > 0:
                docs = await fetch_documents(document_block)
                # –ï—Å–ª–∏ save_files_as_html —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è ‚Äî –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ loop.run_in_executor
                await asyncio.get_running_loop().run_in_executor(
                    None, save_files_as_html, page.url, docs
                )
            else:
                print(f'[WARN] –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ == 0 | {doc_count}')
        except Exception as e:
            print(f'[ERROR üî¥] (–æ—à–∏–±–∫–∞ –±–ª–æ–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) {e}')

        # CLOSE ACCORDION
        await accordion_trigger.scroll_into_view_if_needed()
        await page.wait_for_timeout(200)
        await accordion_trigger.click()
        await page.wait_for_timeout(200)

    except Exception as e:
        print(f'[ERROR üî¥] (–Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ —É—á–∞—Å—Ç–Ω–∏–∫–µ) {e}')


@retry(attempts=80)
async def process_tender_page(page: Page, tender_url: str):
    url = f'https://prozorro.gov.ua/uk{tender_url}'
    await page.goto(url, wait_until="domcontentloaded")

    try:
        title_locator = page.locator('//h2[contains(@class, "title--large")]')
        await title_locator.first.wait_for(timeout=7000)
        title_text = await title_locator.first.text_content()
        if not title_text:
            return None
    except PWTimeout:
        return None

    try:
        participants_locator = page.locator(
            '//section[contains(@class, "register")]//div[contains(@class, "accordion")]'
        )
        await participants_locator.first.wait_for(timeout=5000)
        count = await participants_locator.count()

        if count == 0:
            print(f'[INFO] —Ç–µ–Ω–¥–µ—Ä {url} ‚Äî –Ω–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
            return []
        else:
            print(f'[INFO] —Ç–µ–Ω–¥–µ—Ä {url} ‚Äî {count} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
            return participants_locator
    except PWTimeout:
        print(f'[INFO] —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Üí {page.url}')
        return []


@retry(attempts=60)
async def fetch_tender_links(page: Page, page_index: int) -> Optional[List[str]]:
    url = (
        f"https://prozorro.gov.ua/uk/search/tender?"
        f"cpv=34110000-1&page={page_index}&status=complete&sort=publication_date,asc"
    )
    await page.goto(url, wait_until="domcontentloaded")

    try:
        links_locator = page.locator(
            '//ul[@class="search-result__list"]//a[contains(@class,"item-title__title")]'
        )
        await links_locator.first.wait_for(timeout=15000)

        links = await links_locator.evaluate_all(
            "els => els.map(e => e.getAttribute('href'))"
        )
        links = [l for l in links if l]
        return links if links else None
    except PWTimeout:
        return None


async def process_one_tender(context: BrowserContext, tender_url: str):
    page = await context.new_page()
    try:
        participants = await process_tender_page(page, tender_url)
        if not participants:
            return

        count = await participants.count()
        for i in range(count):
            print(f'[INFO] –û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{count} —É—á–∞—Å—Ç–Ω–∏–∫–∞ ‚Üí {page.url}')
            participant_block = participants.nth(i)
            await process_participant(page, participant_block)

        print("----" * 40)

    except Exception as e:
        print(f"[ERROR] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–Ω–¥–µ—Ä–∞ {tender_url} ‚Üí {e}")
    finally:
        try:
            await page.close()
        except:
            pass


async def run_scraper(start_page: int, end_page: int, headless: bool = True, max_concurrent_tenders: int = 3):
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(
            headless=headless,
            args=["--no-sandbox", "--disable-setuid-sandbox"]
        )
        context = await browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/128.0.0.0 Safari/537.36"
            ),
            locale="uk-UA",
            # bypass_csp=True,   # –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å CSP
        )

        # –°–µ–º—Ñ–æ—Ä –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤
        semaphore = asyncio.Semaphore(max_concurrent_tenders)

        async def bounded_process(tender_url):
            async with semaphore:
                await process_one_tender(context, tender_url)

        for page_index in range(start_page, end_page + 1):
            print(f"\n[PAGE] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ {page_index}")
            search_page = await context.new_page()

            try:
                tender_links = await fetch_tender_links(search_page, page_index)
                if not tender_links:
                    print(f"[WARN] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index} –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    continue

                print(f"[INFO] –ù–∞–π–¥–µ–Ω–æ —Ç–µ–Ω–¥–µ—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index}: {len(tender_links)}")
            finally:
                await search_page.close()

            if tender_links:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–Ω–¥–µ—Ä–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Å–µ–º–∞—Ñ–æ—Ä–æ–º)
                tasks = [bounded_process(url) for url in tender_links]
                await asyncio.gather(*tasks, return_exceptions=True)

        await context.close()
        await browser.close()


if __name__ == "__main__":
    import asyncio

    HEADLESS = True
    START_PAGE = 290
    END_PAGE = 300
    MAX_CONCURRENT_TENDERS = 10

    start_time = time.time()
    asyncio.run(run_scraper(
        start_page=START_PAGE,
        end_page=END_PAGE,
        headless=HEADLESS,
        max_concurrent_tenders=MAX_CONCURRENT_TENDERS
    ))
    end_time = time.time()
    elapsed = end_time - start_time
    print(f"[INFO] –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {elapsed:.2f} —Å–µ–∫—É–Ω–¥ ({elapsed/60:.2f} –º–∏–Ω—É—Ç)")